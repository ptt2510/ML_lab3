# -*- coding: utf-8 -*-
"""lab3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZwgmrXJeFtMSlOzaUdoRwjmWhzFvfAmG

**BÀI 1**
"""

link='https://www.kaggle.com/datasets/camnugent/california-housing-prices'

!pip install wget

import wget
wget.download(link)

import pandas as pd
data=pd.read_csv('/content/sample_data/california_housing_train.csv')

data=pd.DataFrame(data)

datalb=data["median_house_value"]
data = data.drop(["median_house_value"], axis='columns')

import numpy as np
data=np.array(data)

data_b = np.c_[np.ones((17000, 1)), data]

theta_best = np.linalg.inv(data_b.T.dot(data_b)).dot(data_b.T).dot(datalb)

theta_best

data_test=pd.read_csv('/content/sample_data/california_housing_test.csv')

import numpy as np
data_test.replace([np.inf, -np.inf], np.nan, inplace=True)
data_test.dropna(inplace=True)

data_test=pd.DataFrame(data_test)

datalb_test=data_test["median_house_value"]
data_test = data_test.drop(["median_house_value"], axis='columns')

data_test=np.array(data_test)

data_test.shape

data_test_b=np.c_[np.ones((3000, 1)), data_test]

y_predict=data_test_b.dot(theta_best)

datalb_test[0]

y_predict[0]

"""# **BÀI 2**"""

import zipfile
with zipfile.ZipFile("/content/drive/MyDrive/predict+students+dropout+and+academic+success.zip", 'r') as zip_ref:
    zip_ref.extractall("/content/sample_data")

import pandas as pd
data2=pd.read_csv("/content/sample_data/data.csv")

data2=pd.DataFrame(data2)

data=[]
for i in data2.values:
  for a in i:
    data.append(a.split(';'))

data=pd.DataFrame(data)

for i in range (len(data)):
  a=data[36][i]
  if a=="Graduate":
    data[36][i]=0
  else:
    data[36][i]=1

datalb=[]
for i in range (len(data)):
  a=data[36][i]
  datalb.append(a)

datalb=pd.DataFrame(datalb)

data = data.drop(36, axis=1)

def logistic_function(x):
    return 1/ (1 + np.exp(-x))

def compute_cost(theta, x, y):
    m = len(y)
    y_pred = logistic_function(np.dot(x , theta))
    error = (y * np.log(y_pred)) + ((1 - y) * np.log(1 - y_pred))
    cost = -1 / m * sum(error)
    gradient = 1 / m * np.dot(x.T, (y_pred - y))
    return cost , gradient

def gradient_descent(x, y, theta, alpha, iterations):
    costs = []
    for i in range(iterations):
        cost, gradient = compute_cost(theta, x, y)
        theta -= (alpha * gradient)
        costs.append(cost)

    return theta, costs

data=pd.DataFrame(data,dtype=float)

import numpy as np

mean_scores = np.mean(data.values, axis=0)
std_scores = np.std(data.values, axis=0)
scores = (data.values - mean_scores) / std_scores

rows = scores.shape[0]
cols = scores.shape[1]

X = np.append(np.ones((rows, 1)), scores, axis=1)
y = datalb.values.reshape(rows, 1)

theta_init = np.zeros((cols + 1, 1))
cost, gradient = compute_cost(theta_init, X, y)

print("Cost at initialization", cost)
print("Gradient at initialization:", gradient)

theta, costs = gradient_descent(X, y, theta_init, 0.2, 500)

from matplotlib import pyplot as plt
plt.plot(costs)
plt.xlabel("Iterations")
plt.ylabel("$J(\Theta)$")
plt.title("Values of Cost Function over iterations of Gradient Descent");

def predict(theta, x):
    results = np.dot(x,theta)
    return results>0

p = predict(theta, X)
p=pd.DataFrame(p)
p=p.replace([True],1)
p=p.replace([False],0)

p=np.array(p)
datalbi=np.array(datalb.values)

c= np.sum(p==datalbi)

print("Training Accuracy:", str((c/data.shape[0])*100),"%")

"""**BÀI 3**"""

import pandas as pd
data3=pd.read_csv("/content/sample_data/data.csv")

data3=pd.DataFrame(data3)

data=[]
for i in data3.values:
  for a in i:
    data.append(a.split(';'))

data=pd.DataFrame(data)

for i in range (len(data)):
  a=data[36][i]
  if a=="Graduate":
    data[36][i]=0
  elif a=="Dropout":
    data[36][i]=1
  else:
    data[36][i]=2

datalb=[]
for i in range (len(data)):
  a=data[36][i]
  datalb.append(a)

datalb=pd.DataFrame(datalb)

data = data.drop(36, axis=1)

datalb=np.array(datalb)

N = data.shape[0]
d = data.shape[1]
C = 3

y = np.random.randint(0, 3, (N,))

for i in range(0,len(datalb)):
  y[i]=datalb[0][i]

from scipy import sparse
def convert_labels(y, C = C):
    Y = sparse.coo_matrix((np.ones_like(y), (y, np.arange(len(y)))), shape = (C, len(y))).toarray()
    return Y

Y = convert_labels(y, C)

m=data.shape[0]

def cost(X, Y, W):
    A = softmax(W.T.dot(X))
    return -np.sum(Y*np.log(A))

def grad(X, Y, W):
    A = softmax((W.T.dot(X)))
    E = A - Y
    return np.dot(X.T,E)/m

def softmax(Z):
    """
    Compute softmax values for each sets of scores in V.
    each column of V is a set of score.
    """
    e_Z = np.exp(Z)
    A = e_Z / e_Z.sum(axis = 0)
    return A

def softmax_stable(Z):
    """
    Compute softmax values for each sets of scores in Z.
    each column of Z is a set of score.
    """
    e_Z = np.exp(Z - np.max(Z, axis = 0, keepdims = True))
    A = e_Z / e_Z.sum(axis = 0)
    return A

def pred(W, X):
    A = softmax_stable(W.T.dot(X))
    return np.argmax(A, axis = 0)

"""**BÀI 4**

**Linear Regression**
"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

data=pd.read_csv('/content/sample_data/california_housing_train.csv')

data=pd.DataFrame(data)

datalb=data["median_house_value"]
data = data.drop(["median_house_value"], axis='columns')

X = np.array(data)
y = np.array(datalb)

X.shape

y.shape

test=pd.read_csv('/content/sample_data/california_housing_test.csv')

test=pd.DataFrame(test)

testlb=test["median_house_value"]
test = test.drop(["median_house_value"], axis='columns')

X_test = np.array(test)
y_test = np.array(testlb)

regr = LinearRegression()
regr.fit(X, y)
y_pred = regr.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("Mean Squared Error:", mse)
print("R-squared:", r2)

"""***Logistic Regression***"""

data3=pd.read_csv("/content/sample_data/data.csv")

data3=pd.DataFrame(data3)
data=[]
for i in data3.values:
  for a in i:
    data.append(a.split(';'))
data=pd.DataFrame(data)
for i in range (len(data)):
  a=data[36][i]
  if a=="Graduate":
    data[36][i]=0
  else:
    data[36][i]=1
datalb=[]
for i in range (len(data)):
  a=data[36][i]
  datalb.append(a)
data = data.drop(36, axis=1)
datalb=np.array(datalb)
data=np.array(data.values)

X_train, X_test, y_train, y_test = train_test_split(data, datalb, test_size=0.2)

from sklearn.linear_model import LogisticRegression
 log_reg = LogisticRegression()
 log_reg.fit(X_train, y_train)

y_pre = log_reg.predict(X_test)

from sklearn.metrics import accuracy_score
print('Training size = %d, accuracy = %.2f%%' % \
      (X_train.shape[0],accuracy_score(y_test, y_pre)*100))

"""**Softmax Regression**"""

data3=pd.DataFrame(data3)
data=[]
for i in data3.values:
  for a in i:
    data.append(a.split(';'))
data=pd.DataFrame(data)
for i in range (len(data)):
  a=data[36][i]
  if a=="Graduate":
    data[36][i]=0
  elif a=="Dropout":
    data[36][i]=1
  else:
    data[36][i]=2
datalb=[]
for i in range (len(data)):
  a=data[36][i]
  datalb.append(a)
data = data.drop(36, axis=1)
datalb=np.array(datalb)
data=np.array(data.values)

X_train, X_test, y_train, y_test = train_test_split(data, datalb, test_size=0.2)

softmax_reg = LogisticRegression(multi_class="multinomial",solver="lbfgs", C=3)
 softmax_reg.fit(X_train, y_train)

y_pre = softmax_reg.predict(X_test)

from sklearn.metrics import accuracy_score
print('Training size = %d, accuracy = %.2f%%' % \
      (X_train.shape[0],accuracy_score(y_test, y_pre)*100))